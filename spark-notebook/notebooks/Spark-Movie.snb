{
  "metadata" : {
    "name" : "Visualization",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [ "org.apache.kafka %% kafka % 0.8.2.1", "org.apache.kafka % kafka-clients % 0.11.0.0", "org.apache.spark %% spark-core % 2.1.1", "org.apache.spark %% spark-streaming % 2.1.1", "org.apache.spark %% spark-streaming-kafka-0-8 % 2.1.1", "com.typesafe.play %% play-json % 2.4.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "C86DB2E757514494BDCAC81D39E7CA1A"
    },
    "cell_type" : "markdown",
    "source" : "# Spark Movie Notebook: Sentiment Analysis"
  }, {
    "metadata" : {
      "id" : "B142C249273549C5B3674E7840897A1A"
    },
    "cell_type" : "markdown",
    "source" : "## Includes Dependencies"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "002FD47C8387498C8898D38143240A59"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.SparkConf\nimport org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.streaming.StreamingContext._\nimport kafka.serializer.StringDecoder\nimport org.apache.kafka.common.serialization.StringDeserializer\n\nimport play.api.libs.json.Json\n\nimport notebook.front.widgets.magic\nimport notebook.front.widgets.magic._\nimport notebook.front.widgets.magic.Implicits._\nimport notebook.front.widgets.magic.SamplerImplicits._\n\nimport scala.collection.parallel.mutable.ParHashMap",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.SparkConf\nimport org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.streaming.StreamingContext._\nimport kafka.serializer.StringDecoder\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport play.api.libs.json.Json\nimport notebook.front.widgets.magic\nimport notebook.front.widgets.magic._\nimport notebook.front.widgets.magic.Implicits._\nimport notebook.front.widgets.magic.SamplerImplicits._\nimport scala.collection.parallel.mutable.ParHashMap\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1,
      "time" : "Took: 5 seconds 465 milliseconds, at 2017-7-9 19:11"
    } ]
  }, {
    "metadata" : {
      "id" : "C0E0406836F047ECBB2D9B0EC96B42F4"
    },
    "cell_type" : "markdown",
    "source" : "## Json Case Class"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C21023AFFE0742FE86B341B55DD91620"
    },
    "cell_type" : "code",
    "source" : "\nobject Review extends Serializable {\n  case class Review (\n    author : String,\n    id : String,\n    url : String,\n    content : String,\n    analysis : Int\n  ) extends Serializable \n  @transient implicit val Reads = Json.reads[Review]\n}\n\nobject MovieRaw extends Serializable  {\n  case class MovieRaw (\n    id : Int,\n    original_title : String,\n    title : String,\n    overview : String,\n    original_language : String,\n    release_date : String,\n    review_popularity : Float,\n    popularity : Float,\n    vote_average : Float,\n    vote_count : Int,\n    reviews : List[Review.Review],\n    genre_ids : List[Int],\n    video : Boolean\n  ) extends Serializable\n  @transient implicit val Reads = Json.reads[MovieRaw]\n}\n\ndef stringToMovie(str: String) : Option[MovieRaw.MovieRaw] = {\n  try {\n    Some(Json.fromJson[MovieRaw.MovieRaw](Json.parse(str)).get)\n  } catch {\n    case e: Exception => {print(\"x1\"); None}\n  }\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined object Review\ndefined object MovieRaw\nstringToMovie: (str: String)Option[MovieRaw.MovieRaw]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 7 seconds 928 milliseconds, at 2017-7-9 19:11"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C79734619313483F99154AF729C1AC8D"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 6 seconds 818 milliseconds, at 2017-7-9 19:11"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EE16964E4E444B4E837068EE8DBCB37F"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 5 seconds 258 milliseconds, at 2017-7-9 19:11"
    } ]
  }, {
    "metadata" : {
      "id" : "1DCAE991FA624E83A5A2D5F513D71748"
    },
    "cell_type" : "markdown",
    "source" : "## Connect To Kafka Data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DE19DEC975324279B14C1782821840C8"
    },
    "cell_type" : "code",
    "source" : "val brokers = \"localhost:9092\"\nval topic = \"movie-analyzed\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "brokers: String = localhost:9092\ntopic: String = movie-analyzed\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 3 seconds 342 milliseconds, at 2017-7-9 19:11"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9FB2D49218394BE1B4B9D52E777F3976"
    },
    "cell_type" : "code",
    "source" : "@transient val ssc = StreamingContext.getActiveOrCreate(() => new StreamingContext(sc, Seconds(10)))\n\n//val kafkaParams = Map[String, String](\"bootstrap.servers\" -> brokers)\nval kafkaParams = Map[String, String] (\n  \"bootstrap.servers\" -> brokers,\n  \"group.id\" -> \"test\",\n  \"auto.offset.reset\" -> \"smallest\"\n)\n\nval stream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, Set(topic))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@25192c4a\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(bootstrap.servers -> localhost:9092, group.id -> test, auto.offset.reset -> smallest)\nstream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@35d1038\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 5 seconds 397 milliseconds, at 2017-7-9 19:12"
    } ]
  }, {
    "metadata" : {
      "id" : "BE700144BF3F437B83E91BC4D258FE42"
    },
    "cell_type" : "markdown",
    "source" : "## Map-Reduce Processing"
  }, {
    "metadata" : {
      "id" : "618DDA4F3EFA4376BF09F1CC56801B41"
    },
    "cell_type" : "markdown",
    "source" : "### Data Container Creation"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CD5E641883C748E48416E7980576AB3E"
    },
    "cell_type" : "code",
    "source" : "val byYearMovie = new ParHashMap[String, Int]()\ndef addToMap(str : String, v : Int, map : ParHashMap[String, Int]) {\n  //byYearMovie(str) = byYearMovie(str) + v\n  if (map.get(str).isEmpty) {\n      map.put(str, 0)\n  }\n  map.put(str, map(str) + v)\n}\n//byYearMovie(\"testtttt\") = byYearMovie(\"testtttt\") + 1",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "byYearMovie: scala.collection.parallel.mutable.ParHashMap[String,Int] = ParHashMap()\naddToMap: (str: String, v: Int, map: scala.collection.parallel.mutable.ParHashMap[String,Int])Unit\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 3 seconds 829 milliseconds, at 2017-7-9 19:12"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3E67AA5A8C664319883A11CA7E2C62B2"
    },
    "cell_type" : "code",
    "source" : "//val toJson = stream.map(_._2).map(Json.parse(_).as[MovieRaw.MovieRaw]).print()\nstream.map(_._2).foreachRDD { rdd => {\n  if (rdd.isEmpty()) {\n    println(\"empty :(\")\n    ssc.stop()\n  }\n  else {\n    //rdd.map(x => x._2)\n    rdd.flatMap(x => {\n      try {\n        print(x)\n        stringToMovie(x)\n      } catch {\n        case e: Exception => {print(\"NOT OK\"); None}\n      }\n    }).count()\n    //.groupBy(x => x.original_language)\n    //.map(x => (x._1, x._2.size))\n    //.collect()\n    //.foreach(x => addToMap(x._1, x._2, byYearMovie))\n  }\n  }\n}\n\nssc.start()\nssc.awaitTermination()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "org.apache.spark.SparkException: Task not serializable\n  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:298)\n  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)\n  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)\n  at org.apache.spark.SparkContext.clean(SparkContext.scala:2101)\n  at org.apache.spark.rdd.RDD$$anonfun$flatMap$1.apply(RDD.scala:379)\n  at org.apache.spark.rdd.RDD$$anonfun$flatMap$1.apply(RDD.scala:378)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.flatMap(RDD.scala:378)\n  at $anonfun$2.apply(<console>:106)\n  at $anonfun$2.apply(<console>:99)\n  at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)\n  at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)\n  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n  at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)\n  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n  at scala.util.Try$.apply(Try.scala:192)\n  at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)\n  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n  at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n  at java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.NotSerializableException: Object of org.apache.spark.streaming.kafka.DirectKafkaInputDStream is being serialized  possibly as a part of closure of an RDD operation. This is because  the DStream object is being referred to from within the closure.  Please rewrite the RDD operation inside this DStream to avoid this.  This has been enforced to avoid bloating of Spark tasks  with unnecessary objects.\nSerialization stack:\n\n  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)\n  ... 30 more\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "65C8CB6640274EFAB6146D1E49C1B464"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 17,
      "time" : "Took: 2 seconds 211 milliseconds, at 2017-7-9 16:53"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F725E7A5818A480C8E071968F0865581"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 18,
      "time" : "Took: 2 seconds 489 milliseconds, at 2017-7-9 16:53"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BEDB5F3F83E64A84B53AC4D978A06239"
    },
    "cell_type" : "code",
    "source" : "ssc.stop()",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 5 seconds 402 milliseconds, at 2017-7-9 18:20"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A43A25061F65407C869F2BA24BC27451"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 20,
      "time" : "Took: 2 seconds 535 milliseconds, at 2017-7-9 16:53"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "E82479293BDD4EE289A3FF3AAF51D0C9"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}