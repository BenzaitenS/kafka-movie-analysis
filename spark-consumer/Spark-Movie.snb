{"metadata":{"name":"Spark-Movie","user_save_timestamp":"1970-01-01T00:00:00.000Z","auto_save_timestamp":"1970-01-01T00:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":["org.apache.spark %% spark-core % 2.1.1","org.apache.spark %% spark-streaming % 2.1.1","org.apache.spark %% spark-streaming-kafka-0-10 % 2.1.1"],"customImports":null,"customArgs":null,"customSparkConf":null},"cells":[{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"29DF225CE96449A6B59C19A6FF78E51A"},"cell_type":"code","source":"import notebook.front.widgets.{BoxPipeComponent, PipeComponent, CustomizableBoxPipe}","outputs":[{"name":"stdout","output_type":"stream","text":"import notebook.front.widgets.{BoxPipeComponent, PipeComponent, CustomizableBoxPipe}\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":1,"time":"Took: 3 seconds 241 milliseconds, at 2017-7-8 11:36"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"F0FB3F586F9A4E8C90AA306E1485D1CB"},"cell_type":"code","source":"import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkFiles\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nimport org.apache.kafka.common.serialization.StringDeserializer","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkFiles\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.kafka.common.serialization.StringDeserializer\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":2,"time":"Took: 3 seconds 23 milliseconds, at 2017-7-8 11:37"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"8CA8F906ACC8426E9A2C09401A080101"},"cell_type":"code","source":"val consume_topic = \"movie-analyzed\"\nval broker = \"192.168.217.132\" + \":9092\"\nval groupId = \"analyzed-group\"","outputs":[{"name":"stdout","output_type":"stream","text":"consume_topic: String = movie-analyzed\nbroker: String = 192.168.217.132:9092\ngroupId: String = analyzed-group\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":19,"time":"Took: 1 second 259 milliseconds, at 2017-7-8 11:45"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"F75457E1B27547FE811E34D750EF8C63"},"cell_type":"code","source":"val kafkaParams = Map[String, Object] (\n  \"bootstrap.servers\" -> broker,\n  \"group.id\" -> groupId,\n  \"auto.offset.reset\" -> \"earliest\",\n  \"key.deserializer\" -> classOf[StringDeserializer],\n  \"value.deserializer\" -> classOf[StringDeserializer]\n)","outputs":[{"name":"stdout","output_type":"stream","text":"kafkaParams: scala.collection.immutable.Map[String,Object] = Map(key.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer, auto.offset.reset -> earliest, group.id -> analyzed-group, bootstrap.servers -> 192.168.217.132:9092, value.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer)\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":20,"time":"Took: 1 second 895 milliseconds, at 2017-7-8 11:45"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"FC1669F84ED04DEF866B1A3B93658C0C"},"cell_type":"code","source":"\nval ssc = new StreamingContext(sparkContext, Seconds(1))\nssc.checkpoint(\"checkpoint\")\n\nval topics = Array(consume_topic)\n\n// Creates the stream connecting the streaming context\n// to several Kafka topics.\nval stream = KafkaUtils.createDirectStream[String, String] (\n  ssc, PreferConsistent,\n  Subscribe[String, String](topics, kafkaParams)\n)","outputs":[{"name":"stdout","output_type":"stream","text":"ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2b4ef631\ntopics: Array[String] = Array(movie-analyzed)\nstream: org.apache.spark.streaming.dstream.InputDStream[org.apache.kafka.clients.consumer.ConsumerRecord[String,String]] = org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@4d05212d\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":21,"time":"Took: 1 second 114 milliseconds, at 2017-7-8 11:45"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"FFA87DEA24B54C7482192DF4D3EB7544"},"cell_type":"code","source":"stream.map(record => (record.key, record.value))\nstream.foreachRDD { rdd =>\n\n  System.out.println(\"TEST\")\n  \n/*rdd.map(rddVal => (rddVal.value))\n  .foreachPartition(partition => {\n\n    partition.foreach {\n      case movieStr : String => {\n        System.out.println(movieStr)\n      }\n    }\n\n  })*/\n\n}\n\nssc.start()\nssc.awaitTermination()","outputs":[{"name":"stdout","output_type":"stream","text":"org.apache.kafka.common.KafkaException: Failed to construct kafka consumer\n  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:702)\n  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:557)\n  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:540)\n  at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:83)\n  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:75)\n  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:243)\n  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$5.apply(DStreamGraph.scala:49)\n  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$5.apply(DStreamGraph.scala:49)\n  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach_quick(ParArray.scala:143)\n  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:136)\n  at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:972)\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n  at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\n  at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:969)\n  at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\n  at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\n  at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\n  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n  at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n  at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:577)\n  at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:571)\n  ... 67 elided\nCaused by: org.apache.kafka.common.KafkaException: org.apache.kafka.clients.consumer.RangeAssignor ClassNotFoundException exception occured\n  at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:227)\n  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:637)\n  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:557)\n  at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:540)\n  at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:83)\n  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:75)\n  at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:243)\n  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$5.apply(DStreamGraph.scala:49)\n  at org.apache.spark.streaming.DStreamGraph$$anonfun$start$5.apply(DStreamGraph.scala:49)\n  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach_quick(ParArray.scala:143)\n  at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:136)\n  at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:972)\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\n  at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\n  at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:969)\n  at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\n  at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\n  at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\n  ... 4 more\nCaused by: java.lang.ClassNotFoundException: org.apache.kafka.clients.consumer.RangeAssignor\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:359)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:348)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:347)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n  at java.lang.Class.forName0(Native Method)\n  at java.lang.Class.forName(Class.java:278)\n  at org.apache.kafka.common.utils.Utils.newInstance(Utils.java:332)\n  at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:225)\n  ... 23 more\n"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"F5CBAD4D278D4D6B8DC4D531F0BEE854"},"cell_type":"code","source":"val f:Flow = Flow()\nf.update(\"f\",  $intp)\nval result = f.run {\n  case (\"58379e31-1d09-4e85-983c-315f07f4119d\", _) => Map(\"in\" → sparkContext.parallelize(1 to 100))\n  case (\"9c6d5011-3a88-4928-b264-a230d260bf75\", _) => Map(\"in\" → sparkContext.parallelize(2 to 200 by 2))\n  case _ => Map.empty\n}\n\n// Gets results\nval array:Array[(String, Int, Int)] = result((\"6834753e-01f9-49e7-b2ca-d6f8f84e2e2d\", \"collect\")).\n                                      asInstanceOf[Array[(String, (Int, Int))]].\n                                      map{ case (i, (j,k)) => (i,j,k) }","outputs":[{"name":"stdout","output_type":"stream","text":"java.util.NoSuchElementException: key not found: (6834753e-01f9-49e7-b2ca-d6f8f84e2e2d,collect)\n  at scala.collection.MapLike$class.default(MapLike.scala:228)\n  at scala.collection.AbstractMap.default(Map.scala:59)\n  at scala.collection.MapLike$class.apply(MapLike.scala:141)\n  at scala.collection.AbstractMap.apply(Map.scala:59)\n  ... 67 elided\n"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"70AB4D6935334276975EDED423DC3D76"},"cell_type":"code","source":"f","outputs":[{"name":"stdout","output_type":"stream","text":"<console>:84: error: not found: value f\n       f\n       ^\n"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"E93DAE815F104B84AEDA2BB4FDF3439B"},"cell_type":"code","source":"","outputs":[]}],"nbformat":4}